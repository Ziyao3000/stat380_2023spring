---
title: "Homework 4"
author: "[Ziyao Yang]{style='background-color: yellow;'}"
toc: true
title-block-banner: true
title-block-style: default
format: html
# format: pdf
---

[Link to the Github repository](https://github.com/psu-stat380/hw-4)

---

::: {.callout-important style="font-size: 0.8em;"}
## Due: Sun, Apr 2, 2023 @ 11:59pm

Please read the instructions carefully before submitting your assignment.

1. This assignment requires you to only upload a `PDF` file on Canvas
1. Don't collapse any code cells before submitting. 
1. Remember to make sure all your code output is rendered properly before uploading your submission.

⚠️ Please add your name to the author information in the frontmatter before submitting your assignment ⚠️
:::


We will be using the following libraries:

```{R}
packages <- c(
  "dplyr", 
  "readr", 
  "tidyr", 
  "purrr", 
  "stringr", 
  "corrplot", 
  "car", 
  "caret", 
  "torch", 
  "nnet", 
  "broom"
)

# renv::install(packages)
sapply(packages, require, character.only=T)
```

<br><br><br><br>
---

## Question 1
::: {.callout-tip}
## 30 points
Automatic differentiation using `torch`
:::

###### 1.1 (5 points)

Consider $g(x, y)$ given by
$$
g(x, y) = (x - 3)^2 + (y - 4)^2.
$$

Using elementary calculus derive the expressions for

$$
\frac{d}{dx}g(x, y), \quad \text{and} \quad \frac{d}{dy}g(x, y).
$$
\
$$
g(x, y) = (x - 3)^2 + (y - 4)^2 = x^2 -6x +9 + y^2 - 8x +16 = x^2 -6x + y^2 - 8y +25 \\
$$
$$
\frac{d}{dx}g(x, y) = 2x-6
$$
$$
\frac{d}{dy}g(x, y) = 2y-8.
$$
\
Using your answer from above, what is the answer to
$$
\frac{d}{dx}g(x, y) \Bigg|_{(x=3, y=4)} \quad \text{and} \quad \frac{d}{dy}g(x, y) \Bigg|_{(x=3, y=4)} ?
$$
\
$$
\frac{d}{dx}g(x, y) \Bigg|_{(x=3, y=4)} = 2*3-6 = 0
$$
$$
\frac{d}{dy}g(x, y) \Bigg|_{(x=3, y=4)}  = 2*4-8 = 0
$$

Define $g(x, y)$ as a function in R, compute the gradient of $g(x, y)$ with respect to $x=3$ and $y=4$. Does the answer match what you expected?

```{R}
g <- function(x, y) {
  return((x - 3)^2 + (y - 4)^2)
}

# Create torch tensors for the input values (3, 4)
x_val <- torch_tensor(3, requires_grad = TRUE)
y_val <- torch_tensor(4, requires_grad = TRUE)

# Compute the output value of g(x, y)
z <- g(x_val, y_val)

# Perform backward pass to compute gradients
z$backward()

# Get the gradients of g(x, y) with respect to x and y
g_grad_x <- x_val$grad$item()
g_grad_y <- y_val$grad$item()

cat("Gradient with respect to x =", g_grad_x, "\n")
cat("Gradient with respect to y =", g_grad_y, "\n")
```

* The result is the same as I calculated.

---

###### 1.2 (10 points)

$$\newcommand{\u}{\boldsymbol{u}}\newcommand{\v}{\boldsymbol{v}}$$

Consider $h(\u, \v)$ given by
$$
h(\u, \v) = (\u \cdot \v)^3,
$$
where $\u \cdot \v$ denotes the dot product of two vectors, i.e., $\u \cdot \v = \sum_{i=1}^n u_i v_i.$

Using elementary calculus derive the expressions for the gradients

$$
\begin{aligned}
\nabla_\u h(\u, \v) &= \Bigg(\frac{d}{du_1}h(\u, \v), \frac{d}{du_2}h(\u, \v), \dots, \frac{d}{du_n}h(\u, \v)\Bigg)
\end{aligned}
$$
\
$$
\frac{d}{du_i}h(\u_i, \v_i) = \frac{d}{du}(\u_i \cdot \v_i)^3 = 3(\u_i \cdot \v_i)^2 *v_i
$$
Because $\u \cdot \v = \sum_{i=1}^n u_i v_i,$ then
$$
\nabla_\u h(\u, \v) = 3\left(\sum_{i=1}^n u_i v_i\right)^2 \cdot (v_1, v_2, \dots, v_n)
$$
Using your answer from above, what is the answer to $\nabla_\u h(\u, \v)$ when $n=10$ and

$$
\begin{aligned}
\u = (-1, +1, -1, +1, -1, +1, -1, +1, -1, +1)\\
\v = (-1, -1, -1, -1, -1, +1, +1, +1, +1, +1)
\end{aligned}
$$

Define $h(\u, \v)$ as a function in R, initialize the two vectors $\u$ and $\v$ as `torch_tensor`s. Compute the gradient of $h(\u, \v)$ with respect to $\u$. Does the answer match what you expected?
```{R}
h <-  function(u, v) {
  return((u * v)^3)
}

u <- c(-1, +1, -1, +1, -1, +1, -1, +1, -1, +1)
v <- c(-1, -1, -1, -1, -1, +1, +1, +1, +1, +1)

u_val <- torch_tensor(u, requires_grad = TRUE)
v_val <- torch_tensor(v, requires_grad = TRUE)

z <- h(u_val, v_val)

# Create a tensor filled with ones with the same shape as z
ones_tensor <- torch_ones(z$size())

# Compute the gradients for each element in the output tensor
z$backward(ones_tensor)

# Get the gradients of h(u, v) with respect to u and v
h_grad_u <- as_array(u_val$grad)
h_grad_v <- as_array(v_val$grad)

cat("Gradient with respect to u =", h_grad_u, "\n")
cat("Gradient with respect to v =", h_grad_v, "\n")
```
* The result matches what I expected.

---

###### 1.3 (5 points)

Consider the following function
$$
f(z) = z^4 - 6z^2 - 3z + 4
$$

Derive the expression for 
$$
f'(z_0) = \frac{df}{dz}\Bigg|_{z=z_0} = 4z_0^3 -12z_0 -3
$$
and evaluate $f'(z_0)$ when $z_0 = -3.5$.
$$
f'(-3.5) = 4*(-3.5)^3 -12*(-3.5) -3 = -132.5
$$
Define $f(z)$ as a function in R, and using the `torch` library compute $f'(-3.5)$. 
```{R}
f <- function(z){
  return(z^4 - 6*z^2 - 3*z + 4)
}
z_val <- torch_tensor(-3.5, requires_grad = TRUE)

output_val <- f(z_val)
output_val$backward()

f_grad_z <- z_val$grad$item()
cat("Gradient with respect to z_0 = ", f_grad_z, "\n")
```

---

###### 1.4 (5 points)

For the same function $f$, initialize $z[1] = -3.5$, and perform $n=100$ iterations of **gradient descent**, i.e., 

> $z[{k+1}] = z[k] - \eta f'(z[k]) \ \ \ \ $ for $k = 1, 2, \dots, 100$

Plot the curve $f$ and add taking $\eta = 0.02$, add the points $\{z_0, z_1, z_2, \dots z_{100}\}$ obtained using gradient descent to the plot. What do you observe?
```{R}
n = 100
eta = 0.02

z <- numeric(n)
z[1] = -3.5

for (i in 1:100) {
  z_val <- torch_tensor(z[i], requires_grad = TRUE)
  output_val <- f(z_val)
  output_val$backward()
  f_grad_z <- z_val$grad$item()
  z[i+1] = z[i] - eta*f_grad_z
}
```
```{R}
library(ggplot2)
ggplot(data.frame(x = c(-3.5, 2)), aes(x = x)) +
  stat_function(fun = f, color = "red") +
  geom_point(data = data.frame(z_vals = z, f_vals = f(z)), aes(x = z_vals, y = f_vals), color = "blue") +
  labs(x = "z", y = "f(z)", title = "Gradient Descent for f(z), eta = 0.02")+
  scale_x_continuous(breaks = seq(-3.5, 2, by = 0.5))
```
* The points are gradually get closer and closer to and final reach to the local minimum. 


---

###### 1.5 (5 points)

Redo the same analysis as **Question 1.4**, but this time using $\eta = 0.03$. What do you observe? What can you conclude from this analysis
```{R}
n = 100
eta = 0.03

z <- numeric(n)
z[1] = -3.5

for (i in 1:100) {
  z_val <- torch_tensor(z[i], requires_grad = TRUE)
  output_val <- f(z_val)
  output_val$backward()
  f_grad_z <- z_val$grad$item()
  z[i+1] = z[i] - eta*f_grad_z
}
```
```{R}
library(ggplot2)
ggplot(data.frame(x = c(-3.5, 2)), aes(x = x)) +
  stat_function(fun = f, color = "red") +
  geom_point(data = data.frame(z_vals = z, f_vals = f(z)), aes(x = z_vals, y = f_vals), color = "blue") +
  labs(x = "z", y = "f(z)", title = "Gradient Descent for f(z), eta = 0.03")+
  scale_x_continuous(breaks = seq(-3.5, 2, by = 0.5))
```
* Because the learning rate is larger than the rate in question4, the algorithm overshoot the first local minimum point and reaches to the nest local minimum point. And it converge faster(based on the number of dots that other than the minimum point). 


<br><br><br><br>
<br><br><br><br>
---

## Question 2
::: {.callout-tip}
## 50 points
Logistic regression and interpretation of effect sizes
:::

For this question we will use the **Titanic** dataset from the Stanford data archive. This dataset contains information about passengers aboard the Titanic and whether or not they survived. 

---

###### 2.1 (5 points)

Read the data from the following URL as a tibble in R. 
Preprocess the data such that the variables are of the right data type, e.g., binary variables are encoded as factors, and convert all column names to lower case for consistency. 
Let's also rename the response variable `Survival` to `y` for convenience.

```{R}
url <- "https://web.stanford.edu/class/archive/cs/cs109/cs109.1166/stuff/titanic.csv"
df <- as_tibble(read.csv(url))  # Insert your code here
colnames(df) <- tolower(colnames(df))
colnames(df)[which(names(df) == "survived")] <- "y"
is_binary <- function(column) {
  unique_values <- unique(column)
  if (length(unique_values) != 2) {
    return(FALSE)
  }
  TRUE
}
df %>%
  mutate(across(where(is_binary), as.factor))
```

---

###### 2.2 (5 points)

Visualize the correlation matrix of all numeric columns in `df` using `corrplot()`

```{R}
df %>% 
  select_if(is.numeric) %>%
  cor()%>%
  corrplot()
```



---

###### 2.3 (10 points)

Fit a logistic regression model to predict the probability of surviving the titanic as a function of:

* `pclass`
* `sex`
* `age`
* `fare`
* `# siblings`
* `# parents`


```{R}
df <- subset(df, select = -c(name))
full_model <- glm(y~., family = binomial(link = logit), df) # Insert your code here
summary(full_model)
```

---

###### 2.4 (30 points)

Provide an interpretation for the slope and intercept terms estimated in `full_model` in terms of the log-odds of survival in the titanic and in terms of the odds-ratio (if the covariate is also categorical).

::: {.callout-hint}
## 
Recall the definition of logistic regression from the lecture notes, and also recall how we interpreted the slope in the linear regression model (particularly when the covariate was categorical).
:::
* The intercept indicate that if other variables are set to 0, the survival log-odds would be 5.297252. 
  * Meaning that the log-odds of survival for a female passenger in first class who is not traveling with any siblings, spouses, parents, or children, and who has an age and fare of 0 would be 5.297252
  
* The slope indicate that if others remain unchanged, the survival log-odds would increase or decrease by that number, when the x is increase by 1.
  * If others remain unchanged, the survival log-odds would decrease by 1.177659, when the pclass is increase by 1.
  * If others remain unchanged, the survival odds-ratio would decrease by 2.757282, when the sex is male
  * If others remain unchanged, the survival log-odds would decrease by 0.043474, when the age is increase by 1.
  * If others remain unchanged, the survival log-odds would increase by 0.002786, when the fare is increase by 1.
  * If others remain unchanged, the survival log-odds would decrease by 0.401831, when the number of siblings.spouses.aboard is increase by 1.
  * If others remain unchanged, the survival log-odds would decrease by 0.106505, when the number of parents.children.aboard is increase by 1.






<br><br><br><br>
<br><br><br><br>
---

## Question 3
::: {.callout-tip}
## 70 points

Variable selection and logistic regression in `torch`

:::


---

###### 3.1 (15 points)

Complete the following function `overview` which takes in two categorical vectors (`predicted` and `expected`) and outputs:

* The prediction accuracy
* The prediction error
* The false positive rate, and
* The false negative rate

```{R}
expected <- as.factor(df$y)
# Make predictions
probabilities <- full_model %>% 
  predict(df, type = "response")
predicted <- as.factor(ifelse(probabilities > 0.5, "1", "0"))
# Model accuracy
mean(predicted == expected)
#df$predicted <- predicted
table <- table(expected, predicted)
confusionMatrix <- confusionMatrix(data=predicted, reference = expected)
```

```{R}
overview <- function(predicted, expected){
    accuracy <- mean(predicted == expected) # Insert your code here
    error <- 1-accuracy # Insert your code here
    total_false_positives <- table(expected, predicted)[1,2] # Insert your code here
    total_true_positives <- table(expected, predicted)[2,2] # Insert your code here
    total_false_negatives <- table(expected, predicted)[2,1] # Insert your code here
    total_true_negatives <-table(expected, predicted)[1,1]# Insert your code here
    false_positive_rate <- total_false_positives/(total_false_positives+total_true_positives) # Insert your code here
    false_negative_rate <- total_false_negatives/(total_false_negatives+total_true_negatives)# Insert your code here
    return(
        data.frame(
            accuracy = accuracy, 
            error=error, 
            false_positive_rate = false_positive_rate, 
            false_negative_rate = false_negative_rate
        )
    )
}
```
```
overview <- function(predicted, expected){
    accuracy <- mean(predicted == expected) # Insert your code here
    error <- 1-accuracy # Insert your code here
    total_false_positives <- (predicted == 1 && expected == 0) # Insert your code here
    total_true_positives <- (predicted == 1 && expected == 1) # Insert your code here
    total_false_negatives <- (predicted == 0 && expected == 1) # Insert your code here
    total_true_negatives <-(predicted == 0 && expected == 0)# Insert your code here
    false_positive_rate <- total_false_positives/(total_false_positives+total_true_positives) # Insert your code here
    false_negative_rate <- total_false_negatives/(total_false_negatives+total_true_negatives)# Insert your code here
    return(
        data.frame(
            accuracy = accuracy, 
            error=error, 
            false_positive_rate = false_positive_rate, 
            false_negative_rate = false_negative_rate
        )
    )
}
```
You can check if your function is doing what it's supposed to do by evaluating

```{R}
overview(df$y, df$y)
```
and making sure that the accuracy is $100\%$ while the errors are $0\%$.
---

###### 3.2 (5 points)

Display an overview of the key performance metrics of `full_model`

```{R}
overview(predicted, expected) # Insert your code here
```

---

###### 3.3  (5 points)

Using backward-stepwise logistic regression, find a parsimonious alternative to `full_model`, and print its `overview`

```{R}
step_model <- step(full_model, direction = "backward") # Insert your code here. 
summary(step_model)
```

```{R}
probabilities <- step_model %>%
  predict(df, type = "response") # Insert your code here
step_predictions <- as.factor(ifelse(probabilities > 0.5, "1", "0"))
overview(step_predictions, df$y)
```

---

###### 3.4  (15 points)

Using the `caret` package, setup a **$5$-fold cross-validation** training method using the `caret::trainConrol()` function

```{R}
controls <- trainControl(method = "cv", number = 5) # ... insert your code here
```

Now, using `control`, perform $5$-fold cross validation using `caret::train()` to select the optimal $\lambda$ parameter for LASSO with logistic regression. 

Take the search grid for $\lambda$ to be in $\{ 2^{-20}, 2^{-19.5}, 2^{-19}, \dots, 2^{-0.5}, 2^{0} \}$.

```{r}
# Insert your code in the ... region
library(glmnet)
set.seed(123)
#define one-hot encoding function
dummy <- dummyVars(" ~ .", data=df, fullRank = TRUE)
#perform one-hot encoding on data frame
OHE_df <- data.frame(predict(dummy, newdata=df))
OHE_mat <- as.matrix(OHE_df)

lasso_fit <- train(
  x = OHE_mat[,-1],
  y = as.factor(df$y),
  method = "glmnet",
  trControl = controls,
  tuneGrid = expand.grid(
    alpha = 1, #LASSO
    lambda = 2^seq(-20, 0, by = 0.5)
  ),
  family = "binomial"
)
#alpha = 0 and lambda = 0.1767767
```

```{R}
lasso_model <- glmnet(OHE_mat[,-1], df$y, alpha = 1, lambda = lasso_fit$bestTune$lambda)
probabilities <- lasso_model %>%
  predict(OHE_mat[,-1], type = "response")
lasso_predictions <- as.factor(ifelse(probabilities > 0.5, "1", "0"))
overview(lasso_predictions, df$y)
```

Using the information stored in `lasso_fit$results`, plot the results for  cross-validation accuracy vs. $log_2(\lambda)$. Choose the optimal $\lambda^*$, and report your results for this value of $\lambda^*$.
```{R}
plot(lasso_fit)
lasso_fit$results%>%
  filter(Accuracy==max(Accuracy))
```

---

###### 3.5  (25 points)

First, use the `model.matrix()` function to convert the covariates of `df` to a matrix format

```{R}
covariate_matrix <- model.matrix(full_model)[, -1]
```

Now, initialize the covariates $X$ and the response $y$ as `torch` tensors

```{R}
X <- torch_tensor(covariate_matrix, dtype = torch_float()) # Insert your code here
y <- torch_tensor(df$y, dtype = torch_float()) # Insert your code here
```

Using the `torch` library, initialize an `nn_module` which performs logistic regression for this dataset. (Remember that we have 6 different covariates)

```{R}
logistic <- nn_module(
  initialize = function() {
    self$f <- nn_linear(6,1) # 6 covariates, 1 output
    self$g <- nn_sigmoid() # sigmoid function for binary classification
  },
  forward = function(x) {
    x %>% 
      self$f() %>% 
      self$g() # Insert your code here
  }
)

f <- logistic()
```

You can verify that your code is right by checking that the output to the following code is a vector of probabilities:

```{R}
f(X)
```


Now, define the loss function `Loss()` which takes in two tensors `X` and `y` and a function `Fun`, and outputs the **Binary cross Entropy loss** between `Fun(X)` and `y`. 

```{R}
Loss <- function(X, y, Fun){
  y_pred <- f(X)
    return(nn_bce_loss()(y_pred, y)) # Insert our code here
}
```

Initialize an optimizer using `optim_adam()` and perform $n=1000$ steps of gradient descent in order to fit logistic regression using `torch`.

```{R}
f <- logistic()
optimizer <- optim_adam(f$parameters, lr = 0.01) # Insert your code here

n <- 1000
for (i in 1:n){
    loss <- Loss(X, y, f)
    optimizer$zero_grad()
    loss$backward()
    optimizer$step()
    
    if (i %% 100 == 0) {
        cat(sprintf("n: %d, Loss: %.4f\n", i, loss$item()))
    }
}  # Insert your code for gradient descent here
```

Using the final, optimized parameters of `f`, compute the predicted results on `X`

```{R}
predicted_probabilities <- f(X) %>% 
  as_array()

torch_predictions <- ifelse(predicted_probabilities < 0.5, 0, 1) # Insert your code here

overview(torch_predictions, df$y)
```

---

###### 3.6  (5 points)

Create a summary table of the `overview()` summary statistics for each of the $4$ models we have looked at in this assignment, and comment on their relative strengths and drawbacks. 

```{R}
library(tidyverse)

# Get the overview tables for each model
predicted_table <- overview(predicted, df$y) %>%
  mutate(Model = "Full")
step_table <- overview(step_predictions, df$y) %>%
  mutate(Model = "Stepwise")
lasso_table <- overview(lasso_predictions, df$y) %>%
  mutate(Model = "LASSO")
torch_table <- overview(torch_predictions, df$y) %>%
  mutate(Model = "Torch")

# Combine the tables and add names for each model
combined_table <- rbind(
  predicted_table,
  step_table,
  lasso_table,
  torch_table
)

# Print the combined table
combined_table
```
* Among these 4 models, the backward step wise selection model has the highest accuracy, and the model using `torch` has the lowest.
* The Full model is easiest for coding and the `torch` model requites the most work.



:::{.hidden unless-format="pdf"}
\pagebreak
:::

<br><br><br><br>
<br><br><br><br>
---



::: {.callout-note collapse="true"}
## Session Information

Print your `R` session information using the following command

```{R}
sessionInfo()
```
:::